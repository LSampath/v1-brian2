{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.cm as cmap\n",
    "import time\n",
    "import os.path\n",
    "import scipy\n",
    "import pickle as pickle\n",
    "from struct import unpack\n",
    "from brian2 import *\n",
    "import brian2\n",
    "from brian2tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_from_file(fileName):\n",
    "    offset = 4\n",
    "    if fileName[-4-offset] == 'X':\n",
    "        n_src = n_input\n",
    "    else:\n",
    "        if fileName[-3-offset]=='e':\n",
    "            n_src = n_e\n",
    "        else:\n",
    "            n_src = n_i\n",
    "    if fileName[-1-offset]=='e':\n",
    "        n_tgt = n_e\n",
    "    else:\n",
    "        n_tgt = n_i\n",
    "    readout = np.load(fileName)\n",
    "    print(readout.shape, fileName)\n",
    "    value_arr = np.zeros((n_src, n_tgt))\n",
    "    if not readout.shape == (0,):\n",
    "        value_arr[np.int32(readout[:,0]), np.int32(readout[:,1])] = readout[:,2]\n",
    "    return value_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_2d_input_weights(im, fig):\n",
    "    weights = get_2d_input_weights()\n",
    "    im.set_array(weights)\n",
    "    fig.canvas.draw()\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2d_input_weights():\n",
    "    name = 'XeAe'\n",
    "    weight_matrix = np.zeros((n_input, n_e))\n",
    "    n_e_sqrt = int(np.sqrt(n_e))\n",
    "    n_in_sqrt = int(np.sqrt(n_input))\n",
    "    num_values_col = n_e_sqrt*n_in_sqrt\n",
    "    num_values_row = num_values_col\n",
    "    rearranged_weights = np.zeros((num_values_col, num_values_row))\n",
    "    connMatrix = np.zeros((n_input, n_e))\n",
    "    connMatrix[connections[name].i, connections[name].j] = connections[name].w\n",
    "    weight_matrix = np.copy(connMatrix)\n",
    "    \n",
    "    for i in range(n_e_sqrt):\n",
    "        for j in range(n_e_sqrt):\n",
    "                rearranged_weights[i*n_in_sqrt : (i+1)*n_in_sqrt, j*n_in_sqrt : (j+1)*n_in_sqrt] = weight_matrix[:, i + j*n_e_sqrt].reshape((n_in_sqrt, n_in_sqrt))\n",
    "    return rearranged_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_input_weights():\n",
    "    name = 'XeAe'\n",
    "    weights = get_2d_input_weights()\n",
    "    fig = figure(fig_num, figsize = (18, 18))\n",
    "    im2 = imshow(weights, interpolation = \"nearest\", vmin = 0, vmax = wmax_ee, cmap = cmap.get_cmap('hot_r'))\n",
    "    colorbar(im2)\n",
    "    title('weights of connection' + name)\n",
    "    fig.canvas.draw()\n",
    "    return im2, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(fig_num):\n",
    "    num_evaluations = int(num_examples/update_interval)\n",
    "    time_steps = range(0, num_evaluations)\n",
    "    performance = np.zeros(num_evaluations)\n",
    "    fig = figure(fig_num, figsize = (5, 5))\n",
    "    fig_num += 1\n",
    "    ax = fig.add_subplot(111)\n",
    "    im2, = ax.plot(time_steps, performance) #my_cmap\n",
    "    ylim(ymax = 100)\n",
    "    title('Classification performance')\n",
    "    fig.canvas.draw()\n",
    "    return im2, performance, fig_num, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_assignments(result_monitor, input_numbers):\n",
    "    assignments = np.zeros(n_e)\n",
    "    input_nums = np.asarray(input_numbers)        # convers list into numpy.ndarray\n",
    "    maximum_rate = [0] * n_e\n",
    "    for j in range(10):\n",
    "        num_assignments = len(np.where(input_nums == j)[0])\n",
    "        if num_assignments > 0:\n",
    "            rate = np.sum(result_monitor[input_nums == j], axis = 0) / num_assignments\n",
    "        for i in range(n_e):\n",
    "            if rate[i] > maximum_rate[i]:\n",
    "                maximum_rate[i] = rate[i]\n",
    "                assignments[i] = j\n",
    "    return assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_connections(ending=''):\n",
    "    print('save connections')\n",
    "    for connName in save_conns:\n",
    "        conn = connections[connName]\n",
    "        connListSparse = zip(conn.i[:], conn.j[:], conn.w[:])\n",
    "        print(connListSparse)\n",
    "        np.save(data_path + 'weights/' + connName + ending, connListSparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_theta(ending=''):\n",
    "    print('save theta')\n",
    "    for pop_name in population_names:\n",
    "        np.save(data_path + 'weights/theta_' + pop_name + ending, neuron_groups[pop_name + 'e'].theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labeled_data(picklename, bTrain = True):\n",
    "    \"\"\"Read input-vector (image) and target class (label, 0-9) and return\n",
    "       it as list of tuples.\n",
    "    \"\"\"\n",
    "    if os.path.isfile('%s.pickle' % picklename):\n",
    "        data = pickle.load(open('%s.pickle' % picklename, 'rb'))\n",
    "    else:\n",
    "        # Open the images with gzip in read binary mode\n",
    "        if bTrain:\n",
    "            images = open(MNIST_data_path + 'train-images.idx3-ubyte','rb')\n",
    "            labels = open(MNIST_data_path + 'train-labels.idx1-ubyte','rb')\n",
    "        else:\n",
    "            images = open(MNIST_data_path + 't10k-images.idx3-ubyte','rb')\n",
    "            labels = open(MNIST_data_path + 't10k-labels.idx1-ubyte','rb')\n",
    "        # Get metadata for images\n",
    "        images.read(4)  # skip the magic_number\n",
    "        number_of_images = unpack('>I', images.read(4))[0]\n",
    "        rows = unpack('>I', images.read(4))[0]\n",
    "        cols = unpack('>I', images.read(4))[0]\n",
    "        # Get metadata for labels\n",
    "        labels.read(4)  # skip the magic_number\n",
    "        N = unpack('>I', labels.read(4))[0]\n",
    "\n",
    "        if number_of_images != N:\n",
    "            raise Exception('number of labels did not match the number of images')\n",
    "        # Get the data\n",
    "        x = np.zeros((N, rows, cols), dtype=np.uint8)  # Initialize numpy array\n",
    "        y = np.zeros((N, 1), dtype=np.uint8)  # Initialize numpy array\n",
    "        for i in range(N):\n",
    "            if i % 1000 == 0:\n",
    "                print(\"i: %i\" % i)\n",
    "            x[i] = [[unpack('>B', images.read(1))[0] for unused_col in range(cols)]  for unused_row in range(rows) ]\n",
    "            y[i] = unpack('>B', labels.read(1))[0]\n",
    "\n",
    "        data = {'x': x, 'y': y, 'rows': rows, 'cols': cols}\n",
    "        pickle.dump(data, open(\"%s.pickle\" % picklename, \"wb\"), encoding='bytes')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_weights():\n",
    "    for connName in connections:\n",
    "        if connName[1] == 'e' and connName[3] == 'e':\n",
    "            len_source = len(connections[connName].source)\n",
    "            len_target = len(connections[connName].target)\n",
    "            connection = np.zeros((len_source, len_target))\n",
    "            connection[connections[connName].i, connections[connName].j] = connections[connName].w\n",
    "            temp_conn = np.copy(connection)\n",
    "            colSums = np.sum(temp_conn, axis = 0)\n",
    "            colFactors = weight['ee_input']/colSums\n",
    "            for j in range(n_e):#\n",
    "                temp_conn[:,j] *= colFactors[j]\n",
    "            connections[connName].w = temp_conn[connections[connName].i, connections[connName].j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recognized_number_ranking(assignments, spike_rates):\n",
    "    summed_rates = [0] * 10\n",
    "    num_assignments = [0] * 10\n",
    "    for i in range(10):\n",
    "        num_assignments[i] = len(np.where(assignments == i)[0])\n",
    "        if num_assignments[i] > 0:\n",
    "            summed_rates[i] = np.sum(spike_rates[assignments == i]) / num_assignments[i]\n",
    "    return np.argsort(summed_rates)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_data_path = './MINST/'\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "data_path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "training = get_labeled_data(MNIST_data_path + 'training')\n",
    "end = time.time()\n",
    "print('time needed to load training set:', end - start)\n",
    "\n",
    "start = time.time()\n",
    "testing = get_labeled_data(MNIST_data_path + 'testing', bTrain = False)\n",
    "end = time.time()\n",
    "print('time needed to load test set:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is for training\n",
    "\n",
    "weight_path = './test_random/'\n",
    "num_examples = 60000 * 3                  # ???? why 3 times, need to redure 60000 to lower\n",
    "do_plot_performance = True\n",
    "record_spikes = True                      # explicit True\n",
    "ee_STDP_on = True\n",
    "\n",
    "n_input = 784\n",
    "n_e = 400\n",
    "n_i = n_e\n",
    "\n",
    "single_example_time = 0.35 * second #\n",
    "resting_time = 0.15 * second\n",
    "\n",
    "v_rest_e = -65. * mV     #\n",
    "v_rest_i = -60. * mV     #\n",
    "v_reset_e = -65. * mV    #\n",
    "v_reset_i = -45. * mV    #\n",
    "v_thresh_e = -52. * mV   #\n",
    "v_thresh_i = -40. * mV   #\n",
    "refrac_e = 5. * ms     #\n",
    "refrac_i = 2. * ms     #\n",
    "\n",
    "weight = {}\n",
    "delay = {}\n",
    "\n",
    "# no idea\n",
    "weight['ee_input'] = 78.\n",
    "delay['ee_input'] = (0*ms, 10*ms) #min and max delay\n",
    "delay['ei_input'] = (0*ms, 5*ms)                 # is not necessary\n",
    "\n",
    "input_intensity = 2.\n",
    "start_input_intensity = input_intensity   \n",
    "\n",
    "tc_pre_ee = 20*ms\n",
    "tc_post_1_ee = 20*ms\n",
    "tc_post_2_ee = 40*ms\n",
    "nu_ee_pre =  0.0001      # learning rate\n",
    "nu_ee_post = 0.01       # learning rate\n",
    "wmax_ee = 1.0\n",
    "\n",
    "save_conns = ['XeAe']\n",
    "population_names = ['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no idea with number of exaples\n",
    "\n",
    "# if num_examples <= 10000:\n",
    "#     update_interval = num_examples\n",
    "#     weight_update_interval = 20\n",
    "# else:\n",
    "#     update_interval = 10000\n",
    "#     weight_update_interval = 100\n",
    "# if num_examples <= 60000:\n",
    "#     save_connections_interval = 10000\n",
    "# else:\n",
    "#     save_connections_interval = 10000\n",
    "#     update_interval = 10000\n",
    "\n",
    "weight_update_interval = 100\n",
    "save_connections_interval = 10000\n",
    "update_interval = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuron Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_eqs_e = '''\n",
    "        dv/dt = ((v_rest_e - v) + g_e*(-v) + g_i*(-100.*mV - v) ) / (100*ms)  : volt (unless refractory)\n",
    "        dg_e/dt = -g_e/(1.0*ms)                                    : 1\n",
    "        dg_i/dt = -g_i/(2.0*ms)                                    : 1\n",
    "        dtheta/dt = -theta/(1e7*ms)                                : volt\n",
    "        dtimer/dt = 0.1                                            : second\n",
    "'''\n",
    "\n",
    "neuron_eqs_i = '''\n",
    "        dv/dt = ((v_rest_i - v) +  g_e*(-v) + g_i*(-85.*mV - v)) / (10*ms)  : volt (unless refractory)\n",
    "        dg_e/dt = -g_e/(1.0*ms)                                    : 1\n",
    "        dg_i/dt = -g_i/(2.0*ms)                                    : 1\n",
    "'''\n",
    "\n",
    "tc_theta = 1e7 * ms\n",
    "theta_plus_e = 0.05 * mV\n",
    "\n",
    "v_thresh_e_str = '(v > (theta - 20.0*mV + v_thresh_e)) and (timer > refrac_e)'\n",
    "v_thresh_i_str = 'v > v_thresh_i'\n",
    "\n",
    "scr_e = 'v = v_reset_e; theta += theta_plus_e; timer = 0*ms'\n",
    "v_reset_i_str = 'v=v_reset_i'\n",
    "\n",
    "neuron_groups = {}\n",
    "\n",
    "neuron_groups['Ae'] = NeuronGroup(n_e, neuron_eqs_e, threshold= v_thresh_e_str, refractory= refrac_e, reset= scr_e, method='euler')\n",
    "neuron_groups['Ai'] = NeuronGroup(n_i, neuron_eqs_i, threshold= v_thresh_i_str, refractory= refrac_i, reset= v_reset_i_str, method='euler')\n",
    "\n",
    "neuron_groups['Ae'].v = v_rest_e - 40. * mV\n",
    "neuron_groups['Ai'].v = v_rest_i - 40. * mV\n",
    "\n",
    "neuron_groups['Ae'].theta = np.ones((n_e)) * 20.0*mV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create network population and recurrent connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = {}\n",
    "rate_monitors = {}\n",
    "spike_monitors = {}\n",
    "spike_counters = {}\n",
    "input_groups = {}\n",
    "result_monitor = np.zeros((update_interval,n_e))    # (10000, 400)\n",
    "\n",
    "eqs_stdp_ee = '''\n",
    "dpre/dt   =   -pre/(tc_pre_ee)         : 1 (event-driven)\n",
    "dpost1/dt  = -post1/(tc_post_1_ee)     : 1 (event-driven)\n",
    "dpost2/dt  = -post2/(tc_post_2_ee)     : 1 (event-driven)\n",
    "'''\n",
    "eqs_stdp_pre_ee = 'pre = 1.; w = clip(w + nu_ee_pre * post1, 0, wmax_ee)'\n",
    "eqs_stdp_post_ee = 'post1 = 1.; post2 = 1.; w = clip(w + nu_ee_post * pre * post2, 0, wmax_ee)'\n",
    "\n",
    "# AeAi\n",
    "weightMatrix = get_matrix_from_file(weight_path + '../test_random/AeAi.npy')\n",
    "model = 'w : 1'\n",
    "pre = ''\n",
    "post = ''\n",
    "\n",
    "connections['AeAi'] = Synapses(neuron_groups['Ae'], neuron_groups['Ai'], model=model, on_pre=pre, on_post=post)\n",
    "connections['AeAi'].connect(True)   # all-to-all connection\n",
    "connections['AeAi'].w = weightMatrix[connections['AeAi'].i, connections['AeAi'].j]\n",
    "\n",
    "# AiAe\n",
    "weightMatrix = get_matrix_from_file(weight_path + '../test_random/AiAe.npy')\n",
    "model = 'w : 1'\n",
    "pre = ''\n",
    "post = ''\n",
    "\n",
    "connections['AiAe'] = Synapses(neuron_groups['Ai'], neuron_groups['Ae'], model=model, on_pre=pre, on_post=post)\n",
    "connections['AiAe'].connect(True)   # all-to-all connection\n",
    "connections['AiAe'].w = weightMatrix[connections['AiAe'].i, connections['AiAe'].j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('create monitors for A')\n",
    "rate_monitors['Ae'] = PopulationRateMonitor(neuron_groups['Ae'])\n",
    "rate_monitors['Ai'] = PopulationRateMonitor(neuron_groups['Ai'])\n",
    "\n",
    "spike_counters['Ae'] = SpikeMonitor(neuron_groups['Ae'])\n",
    "\n",
    "spike_monitors['Ae'] = SpikeMonitor(neuron_groups['Ae'])\n",
    "spike_monitors['Ai'] = SpikeMonitor(neuron_groups['Ai'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create input population and connections from input populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_groups['Xe'] = PoissonGroup(n_input, 0*Hz)\n",
    "rate_monitors['Xe'] = PopulationRateMonitor(input_groups['Xe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('create connections between X and A')\n",
    "\n",
    "weightMatrix = get_matrix_from_file('./test_random/XeAe.npy')\n",
    "model = 'w : 1'\n",
    "pre = ''\n",
    "post = ''\n",
    "model += eqs_stdp_ee\n",
    "pre += eqs_stdp_pre_ee\n",
    "post = eqs_stdp_post_ee\n",
    "\n",
    "connections['XeAe'] = Synapses(input_groups['Xe'], neuron_groups['Ae'], model=model, on_pre=pre, on_post=post)\n",
    "minDelay = delay['ee_input'][0]\n",
    "maxDelay = delay['ee_input'][1]\n",
    "deltaDelay = maxDelay - minDelay\n",
    "\n",
    "# TODO: test this  ?????????????\n",
    "connections['XeAe'].connect(True) # all-to-all connection\n",
    "connections['XeAe'].delay = 'minDelay + rand() * deltaDelay'\n",
    "connections['XeAe'].w = weightMatrix[connections['XeAe'].i, connections['XeAe'].j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "for obj_list in [neuron_groups, input_groups, connections, rate_monitors, spike_monitors, spike_counters]:\n",
    "    print(obj_list)\n",
    "    for key in obj_list:\n",
    "        print(key)\n",
    "        net.add(obj_list[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_spike_count = np.zeros(n_e)           # (400,)\n",
    "assignments = np.zeros(n_e)                   #  (400,)\n",
    "input_numbers = [0] * num_examples               # array 180000 long\n",
    "outputNumbers = np.zeros((num_examples, 10))    # (180000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_num = 1\n",
    "\n",
    "input_weight_monitor, fig_weights = plot_2d_input_weights()\n",
    "fig_num += 1\n",
    "    \n",
    "performance_monitor, performance, fig_num, fig_performance = plot_performance(fig_num)\n",
    "\n",
    "input_groups['Xe'].rates = 0 * Hz  \n",
    "\n",
    "net.run(0*second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "while j < (int(num_examples)):\n",
    "    normalize_weights()\n",
    "    spike_rates = training['x'][j%60000,:,:].reshape((n_input)) / 8. *  input_intensity  \n",
    "    # (28,28) => (784,)      divide by 4.  0>0Hz and 255>64.925Hz     # input_intensity is 2 ???\n",
    "    \n",
    "    input_groups['Xe'].rates = spike_rates * Hz\n",
    "    print('run example number:', j+1, 'of', int(num_examples))\n",
    "    \n",
    "    net.run(single_example_time, report='text')   # 0.35 s\n",
    "\n",
    "    if j % update_interval == 0 and j > 0:    #update_interval is 10000\n",
    "        assignments = get_new_assignments(result_monitor[:], input_numbers[j-update_interval : j])    #no idea on function\n",
    "        \n",
    "    if j % weight_update_interval == 0:                                    # weight_update_interval 100\n",
    "        update_2d_input_weights(input_weight_monitor, fig_weights)\n",
    "        \n",
    "    if j % save_connections_interval == 0 and j > 0 and not test_mode:    # save_connections_interval = 10000\n",
    "        save_connections(str(j))\n",
    "        save_theta(str(j))\n",
    "\n",
    "    current_spike_count = np.asarray(spike_counters['Ae'].count[:]) - previous_spike_count\n",
    "    previous_spike_count = np.copy(spike_counters['Ae'].count[:])\n",
    "    \n",
    "    if np.sum(current_spike_count) < 5:   # if less than 5 spikes within 350ms, fire rate is increased by 32Hz\n",
    "        input_intensity += 1\n",
    "        input_groups['Xe'].rates = 0 * Hz\n",
    "        net.run(resting_time)                              # 150 ms\n",
    "    \n",
    "    else:                                                   \n",
    "        result_monitor[j%update_interval,:] = current_spike_count    # update_interval = 10000\n",
    "        input_numbers[j] = training['y'][j%60000][0]\n",
    "        \n",
    "        outputNumbers[j,:] = get_recognized_number_ranking(assignments, result_monitor[j%update_interval,:])\n",
    "        \n",
    "        if j % 100 == 0 and j > 0:\n",
    "            print('runs done:', j, 'of', int(num_examples))\n",
    "        if j % update_interval == 0 and j > 0:\n",
    "            if do_plot_performance:\n",
    "                unused, performance = update_performance_plot(performance_monitor, performance, j, fig_performance)\n",
    "                print('Classification performance', performance[:(j/float(update_interval))+1])\n",
    "        for i,name in enumerate(input_population_names):\n",
    "            input_groups[name+'e'].rates = 0 * Hz\n",
    "        net.run(resting_time)\n",
    "        input_intensity = start_input_intensity\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('save results')\n",
    "if not test_mode:\n",
    "    save_theta()\n",
    "if not test_mode:\n",
    "    save_connections()\n",
    "else:\n",
    "    np.save(data_path + 'activity/resultPopVecs' + str(num_examples), result_monitor)\n",
    "    np.save(data_path + 'activity/inputNumbers' + str(num_examples), input_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rate_monitors:\n",
    "    b2.figure(fig_num)\n",
    "    fig_num += 1\n",
    "    for i, name in enumerate(rate_monitors):\n",
    "        b2.subplot(len(rate_monitors), 1, 1+i)\n",
    "        b2.plot(rate_monitors[name].t/b2.second, rate_monitors[name].rate, '.')\n",
    "        b2.title('Rates of population ' + name)\n",
    "\n",
    "if spike_monitors:\n",
    "    b2.figure(fig_num)\n",
    "    fig_num += 1\n",
    "    for i, name in enumerate(spike_monitors):\n",
    "        b2.subplot(len(spike_monitors), 1, 1+i)\n",
    "        b2.plot(spike_monitors[name].t/b2.ms, spike_monitors[name].i, '.')\n",
    "        b2.title('Spikes of population ' + name)\n",
    "\n",
    "if spike_counters:\n",
    "    b2.figure(fig_num)\n",
    "    fig_num += 1\n",
    "    b2.plot(spike_monitors['Ae'].count[:])\n",
    "    b2.title('Spike count of population Ae')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_2d_input_weights()\n",
    "\n",
    "plt.figure(5)\n",
    "\n",
    "subplot(3,1,1)\n",
    "\n",
    "brian_plot(connections['XeAe'].w)\n",
    "subplot(3,1,2)\n",
    "\n",
    "brian_plot(connections['AeAi'].w)\n",
    "\n",
    "subplot(3,1,3)\n",
    "\n",
    "brian_plot(connections['AiAe'].w)\n",
    "\n",
    "\n",
    "plt.figure(6)\n",
    "\n",
    "subplot(3,1,1)\n",
    "\n",
    "brian_plot(connections['XeAe'].delay)\n",
    "subplot(3,1,2)\n",
    "\n",
    "brian_plot(connections['AeAi'].delay)\n",
    "\n",
    "subplot(3,1,3)\n",
    "\n",
    "brian_plot(connections['AiAe'].delay)\n",
    "\n",
    "\n",
    "b2.ioff()\n",
    "b2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_rates = result_monitor[0%update_interval,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_rates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_assignments = [0] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(spike_rates[assignments == 0]) / num_assignments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(spike_rates[assignments == 0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNN simulator",
   "language": "python",
   "name": "sim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
