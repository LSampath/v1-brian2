{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.cm as cmap\n",
    "import time\n",
    "import os.path\n",
    "import scipy\n",
    "import pickle as pickle\n",
    "from struct import unpack\n",
    "from brian2 import *\n",
    "import brian2 as b2\n",
    "from brian2tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_data_path = 'MINST/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labeled_data(picklename, bTrain = True):\n",
    "    \"\"\"Read input-vector (image) and target class (label, 0-9) and return\n",
    "       it as list of tuples.\n",
    "    \"\"\"\n",
    "    if os.path.isfile('%s.pickle' % picklename):\n",
    "        data = pickle.load(open('%s.pickle' % picklename, 'rb'))\n",
    "    else:\n",
    "        # Open the images with gzip in read binary mode\n",
    "        if bTrain:\n",
    "            images = open(MNIST_data_path + 'train-images.idx3-ubyte','rb')\n",
    "            labels = open(MNIST_data_path + 'train-labels.idx1-ubyte','rb')\n",
    "        else:\n",
    "            images = open(MNIST_data_path + 't10k-images.idx3-ubyte','rb')\n",
    "            labels = open(MNIST_data_path + 't10k-labels.idx1-ubyte','rb')\n",
    "        # Get metadata for images\n",
    "        images.read(4)  # skip the magic_number\n",
    "        number_of_images = unpack('>I', images.read(4))[0]\n",
    "        rows = unpack('>I', images.read(4))[0]\n",
    "        cols = unpack('>I', images.read(4))[0]\n",
    "        # Get metadata for labels\n",
    "        labels.read(4)  # skip the magic_number\n",
    "        N = unpack('>I', labels.read(4))[0]\n",
    "\n",
    "        if number_of_images != N:\n",
    "            raise Exception('number of labels did not match the number of images')\n",
    "        # Get the data\n",
    "        x = np.zeros((N, rows, cols), dtype=np.uint8)  # Initialize numpy array\n",
    "        y = np.zeros((N, 1), dtype=np.uint8)  # Initialize numpy array\n",
    "        for i in range(N):\n",
    "            if i % 1000 == 0:\n",
    "                print(\"i: %i\" % i)\n",
    "            x[i] = [[unpack('>B', images.read(1))[0] for unused_col in range(cols)]  for unused_row in range(rows) ]\n",
    "            y[i] = unpack('>B', labels.read(1))[0]\n",
    "\n",
    "        data = {'x': x, 'y': y, 'rows': rows, 'cols': cols}\n",
    "        pickle.dump(data, open(\"%s.pickle\" % picklename, \"wb\"), encoding='bytes')\n",
    "    return data\n",
    "\n",
    "def get_matrix_from_file(fileName):\n",
    "    offset = 4\n",
    "    if fileName[-4-offset] == 'X':\n",
    "        n_src = n_input\n",
    "    else:\n",
    "        if fileName[-3-offset]=='e':\n",
    "            n_src = n_e\n",
    "        else:\n",
    "            n_src = n_i\n",
    "    if fileName[-1-offset]=='e':\n",
    "        n_tgt = n_e\n",
    "    else:\n",
    "        n_tgt = n_i\n",
    "    readout = np.load(fileName)\n",
    "    print(readout.shape, fileName)\n",
    "    value_arr = np.zeros((n_src, n_tgt))\n",
    "    if not readout.shape == (0,):\n",
    "        value_arr[np.int32(readout[:,0]), np.int32(readout[:,1])] = readout[:,2]\n",
    "    return value_arr\n",
    "\n",
    "def get_2d_input_weights():       # (n_in*n_in, n_e*n_e) into (n_in*n_e, n_in*n_e)\n",
    "    name = 'XeAe'\n",
    "    # weight_matrix = np.zeros((n_input, n_e))   # (784, 225) from one pixel to all neurons in 15*15 grid\n",
    "    n_e_sqrt = int(np.sqrt(n_e))\n",
    "    n_in_sqrt = int(np.sqrt(n_input))\n",
    "    num_values_col = n_e_sqrt*n_in_sqrt        # 28*15 = 420\n",
    "    num_values_row = num_values_col\n",
    "    rearranged_weights = np.zeros((num_values_col, num_values_row))           # (420,420) \n",
    "    connMatrix = np.zeros((n_input, n_e))                                     # (784, 225)\n",
    "    connMatrix[connections[name].i, connections[name].j] = connections[name].w   # random values ar begining\n",
    "    weight_matrix = np.copy(connMatrix)\n",
    "    \n",
    "    for i in range(n_e_sqrt):\n",
    "        for j in range(n_e_sqrt):\n",
    "                rearranged_weights[i*n_in_sqrt : (i+1)*n_in_sqrt, j*n_in_sqrt : (j+1)*n_in_sqrt] = weight_matrix[:, i + j*n_e_sqrt].reshape((n_in_sqrt, n_in_sqrt))\n",
    "    return rearranged_weights\n",
    "\n",
    "def plot_2d_input_weights(fig_num):\n",
    "    name = 'XeAe'\n",
    "    weights = get_2d_input_weights()\n",
    "    fig = figure(fig_num, figsize = (10, 10))\n",
    "    im2 = imshow(weights, interpolation = \"nearest\", vmin = 0, vmax = wmax_ee, cmap = cmap.get_cmap('hot_r'))\n",
    "    colorbar(im2)\n",
    "    title('weights of connection' + name)\n",
    "    fig.canvas.draw()\n",
    "    return im2, fig\n",
    "\n",
    "def update_2d_input_weights(im, fig):\n",
    "    weights = get_2d_input_weights()\n",
    "    im.set_array(weights)\n",
    "    fig.canvas.draw()\n",
    "    return im\n",
    "\n",
    "def plot_performance(fig_num):\n",
    "    num_evaluations = int(num_examples/update_interval)\n",
    "    time_steps = range(0, num_evaluations)\n",
    "    performance = np.zeros(num_evaluations)\n",
    "    fig = figure(fig_num, figsize = (5, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im2, = ax.plot(time_steps, performance) #my_cmap\n",
    "    ylim(ymax = 100)\n",
    "    title('Classification performance')\n",
    "    fig.canvas.draw()\n",
    "    return im2, performance, fig_num, fig\n",
    "\n",
    "def update_performance_plot(im, performance, current_example_num, fig):\n",
    "    performance = get_current_performance(performance, current_example_num)\n",
    "    im.set_ydata(performance)\n",
    "    fig.canvas.draw()\n",
    "    return im, performance\n",
    "\n",
    "def normalize_weights():\n",
    "    for connName in connections:                                             # XeAe, AeAi, AiAe\n",
    "        if connName[1] == 'e' and connName[3] == 'e':                        # only normalize XeAe\n",
    "            len_source = len(connections[connName].source)                   # 784 = 28*28\n",
    "            len_target = len(connections[connName].target)                   # 225 = 15*15\n",
    "            connection = np.zeros((len_source, len_target))                  # (784,225)\n",
    "            connection[connections[connName].i, connections[connName].j] = connections[connName].w\n",
    "            temp_conn = np.copy(connection)                                  # why make temp ???????????\n",
    "            colSums = np.sum(temp_conn, axis = 0)\n",
    "            colFactors = weight['ee_input']/colSums                         # 78.0/value of each n_e neuron sum\n",
    "            for j in range(n_e):                  # for each n_e\n",
    "                temp_conn[:,j] *= colFactors[j]   # weight = (weight*78.0) / (sum of input weights for each n_e neuron)\n",
    "            connections[connName].w = temp_conn[connections[connName].i, connections[connName].j]\n",
    "            \n",
    "def get_new_assignments(result_monitor, input_numbers): # assign each n_e neuron, most spiking number\n",
    "    # input_numbers = array of zeros with len of update interval\n",
    "    # result_monitor = np.zeros((update_interval,n_e))     (400, 225) Ae\n",
    "    assignments = np.zeros(n_e)                   # (225,)\n",
    "    input_nums = np.asarray(input_numbers)        # convers list into numpy.ndarray\n",
    "    maximum_rate = [0] * n_e \n",
    "    for j in range(10):\n",
    "        num_assignments = len(np.where(input_nums == j)[0])\n",
    "        if num_assignments > 0:\n",
    "            rate = np.sum(result_monitor[input_nums == j], axis = 0) / num_assignments\n",
    "        for i in range(n_e):\n",
    "            if rate[i] > maximum_rate[i]:\n",
    "                maximum_rate[i] = rate[i]\n",
    "                assignments[i] = j\n",
    "    return assignments\n",
    "\n",
    "def save_connections(ending=''):         # save only XeAe connections\n",
    "    print('save connections')\n",
    "    connName = 'XeAe'\n",
    "    conn = connections[connName]\n",
    "    connListSparse = zip(conn.i[:], conn.j[:], conn.w[:])\n",
    "    print(connListSparse)\n",
    "    np.save('./experiment/weights/' + connName + ending, connListSparse)\n",
    "    \n",
    "def save_theta(ending=''):             # save only A populations\n",
    "    print('save theta')\n",
    "    np.save('./experiment/weights/theta_A' + ending, neuron_groups['Ae'].theta)\n",
    "    \n",
    "def get_recognized_number_ranking(assignments, spike_rates):\n",
    "    summed_rates = [0] * 10\n",
    "    num_assignments = [0] * 10\n",
    "    for i in range(10):\n",
    "        num_assignments[i] = len(np.where(assignments == i)[0])\n",
    "        if num_assignments[i] > 0:\n",
    "            summed_rates[i] = np.sum(spike_rates[assignments == i]) / num_assignments[i]\n",
    "    return np.argsort(summed_rates)[::-1]\n",
    "\n",
    "def get_current_performance(performance, current_example_num):\n",
    "    current_evaluation = int(current_example_num/update_interval)\n",
    "    start_num = current_example_num - update_interval\n",
    "    end_num = current_example_num\n",
    "    difference = outputNumbers[start_num:end_num, 0] - input_numbers[start_num:end_num]\n",
    "    correct = len(np.where(difference == 0)[0])\n",
    "    performance[current_evaluation] = correct / float(update_interval) * 100\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_data_path = './MINST/'\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time needed to load test set: 0.019988536834716797\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "testing = get_labeled_data(MNIST_data_path + 'testing', bTrain = False)\n",
    "end = time.time()\n",
    "print('time needed to load test set:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 100\n",
    "\n",
    "testing['x'] = testing['x'][:dataset_size]\n",
    "testing['y'] = testing['y'][:dataset_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = './experiment/weights'\n",
    "num_examples = dataset_size * 1\n",
    "#use_testing_set = True\n",
    "#do_plot_performance = False\n",
    "#record_spikes = True\n",
    "#ee_STDP_on = False\n",
    "\n",
    "n_input = 784                            # input 28*28 grid\n",
    "n_e = 225                                # 15*15 grid of neurons\n",
    "n_i = n_e\n",
    "\n",
    "single_example_time = 0.35 * second           # run times per example\n",
    "resting_time = 0.15 * second                  # # run times per example\n",
    "\n",
    "weight = {}                                # ?\n",
    "delay = {}                                 # ?\n",
    "\n",
    "weight['ee_input'] = 78.                       # initial weights\n",
    "delay['ee_input'] = (0*ms, 10*ms)              # min and max delay\n",
    "\n",
    "v_rest_e = -65. * mV     #\n",
    "v_rest_i = -60. * mV     #\n",
    "v_reset_e = -65. * mV    #\n",
    "v_reset_i = -45. * mV    #\n",
    "v_thresh_e = -52. * mV   #\n",
    "v_thresh_i = -40. * mV   #\n",
    "refrac_e = 5. * ms     #\n",
    "refrac_i = 2. * ms     #\n",
    "\n",
    "input_intensity = 2.                       # ???\n",
    "start_input_intensity = input_intensity    # input_intensity if set to initial value after each iteration of training\n",
    "\n",
    "tc_pre_ee = 20*ms\n",
    "tc_post_1_ee = 20*ms\n",
    "tc_post_2_ee = 40*ms\n",
    "nu_ee_pre =  0.0001      # learning rate\n",
    "nu_ee_post = 0.01       # learning rate\n",
    "wmax_ee = 1.0\n",
    "\n",
    "weight_update_interval = int(dataset_size / 10)\n",
    "save_connections_interval = int(dataset_size / 5)\n",
    "update_interval = num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_theta = 1e7 * ms\n",
    "theta_plus_e = 0.05 * mV\n",
    "\n",
    "scr_e = 'v = v_reset_e; timer = 0*ms'\n",
    "v_reset_i_str = 'v=v_reset_i'\n",
    "\n",
    "v_thresh_e_str = '(v > (theta - 20.0*mV + v_thresh_e)) and (timer > refrac_e)'\n",
    "v_thresh_i_str = 'v > v_thresh_i'\n",
    "\n",
    "neuron_eqs_e = '''\n",
    "        dv/dt = ((v_rest_e - v) + g_e*(-v) + g_i*(-100.*mV - v) ) / (100*ms)  : volt (unless refractory)\n",
    "        dg_e/dt = -g_e/(1.0*ms)                                    : 1\n",
    "        dg_i/dt = -g_i/(2.0*ms)                                    : 1\n",
    "        theta                                                      :volt\n",
    "        dtimer/dt = 0.1                                            : second\n",
    "'''\n",
    "\n",
    "neuron_eqs_i = '''\n",
    "        dv/dt = ((v_rest_i - v) +  g_e*(-v) + g_i*(-85.*mV - v)) / (10*ms)  : volt (unless refractory)\n",
    "        dg_e/dt = -g_e/(1.0*ms)                                    : 1\n",
    "        dg_i/dt = -g_i/(2.0*ms)                                    : 1\n",
    "'''\n",
    "\n",
    "neuron_groups = {}\n",
    "\n",
    "neuron_groups['Ae'] = NeuronGroup(n_e, neuron_eqs_e, threshold= v_thresh_e_str, refractory= refrac_e, reset= scr_e, method='euler')\n",
    "neuron_groups['Ai'] = NeuronGroup(n_i, neuron_eqs_i, threshold= v_thresh_i_str, refractory= refrac_i, reset= v_reset_i_str, method='euler')\n",
    "\n",
    "neuron_groups['Ae'].v = v_rest_e - 40.*mV\n",
    "neuron_groups['Ai'].v = v_rest_i - 40.*mV\n",
    "\n",
    "# neuron_groups['Ae'].theta = np.ones((n_e)) * 20.0*mV\n",
    "neuron_groups['Ae'].theta = np.load(weight_path + '/theta_A.npy') * volt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225, 3) experiment/random/AeAi.npy\n",
      "(50625, 3) experiment/random/AiAe.npy\n"
     ]
    }
   ],
   "source": [
    "connections = {}                                                      # XeAe, AeAi, AiAe connections (synapses)\n",
    "\n",
    "# AeAi\n",
    "weightMatrix = get_matrix_from_file('experiment/random/AeAi.npy')\n",
    "model = 'w : 1'\n",
    "pre = 'g_e_post += w'\n",
    "post = ''\n",
    "\n",
    "connections['AeAi'] = Synapses(neuron_groups['Ae'], neuron_groups['Ai'], model=model, on_pre=pre, on_post=post)\n",
    "connections['AeAi'].connect(True)                                          # all-to-all connection\n",
    "connections['AeAi'].w = weightMatrix[connections['AeAi'].i, connections['AeAi'].j]\n",
    "\n",
    "# AiAe\n",
    "weightMatrix = get_matrix_from_file('experiment/random/AiAe.npy')\n",
    "model = 'w : 1'\n",
    "pre = 'g_i_post += w'\n",
    "post = ''\n",
    "\n",
    "connections['AiAe'] = Synapses(neuron_groups['Ai'], neuron_groups['Ae'], model=model, on_pre=pre, on_post=post)\n",
    "connections['AiAe'].connect(True)   # all-to-all connection\n",
    "connections['AiAe'].w = weightMatrix[connections['AiAe'].i, connections['AiAe'].j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create monitors\n"
     ]
    }
   ],
   "source": [
    "rate_monitors = {}             # Ae, Ai neuron groups\n",
    "spike_monitors = {}            # Ae, Ai neuron group\n",
    "spike_counters = {}            # Ae neuron group\n",
    "input_groups = {}              # Xe input group, (input groups are not neurons, poission group)\n",
    "\n",
    "result_monitor = np.zeros((update_interval,n_e)) # (400,225) Ae (where number of spikes for each neuron per each image is tracked)\n",
    "\n",
    "print('create monitors')\n",
    "rate_monitors['Ae'] = PopulationRateMonitor(neuron_groups['Ae'])\n",
    "rate_monitors['Ai'] = PopulationRateMonitor(neuron_groups['Ai'])\n",
    "\n",
    "spike_counters['Ae'] = SpikeMonitor(neuron_groups['Ae'])\n",
    "\n",
    "spike_monitors['Ae'] = SpikeMonitor(neuron_groups['Ae'])\n",
    "spike_monitors['Ai'] = SpikeMonitor(neuron_groups['Ai'])\n",
    "\n",
    "input_groups['Xe'] = PoissonGroup(n_input, 0*Hz)\n",
    "rate_monitors['Xe'] = PopulationRateMonitor(input_groups['Xe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create connections between X and A\n",
      "(176400, 3) experiment/random/XeAe.npy\n"
     ]
    }
   ],
   "source": [
    "print('create connections between X and A')\n",
    "\n",
    "\n",
    "weightMatrix = get_matrix_from_file('experiment/random/XeAe.npy')\n",
    "model = 'w : 1'\n",
    "pre = 'g_e_post += w'\n",
    "post = ''\n",
    "\n",
    "connections['XeAe'] = Synapses(input_groups['Xe'], neuron_groups['Ae'], model=model, on_pre=pre, on_post=post)\n",
    "minDelay = delay['ee_input'][0]\n",
    "maxDelay = delay['ee_input'][1]\n",
    "deltaDelay = maxDelay - minDelay                  # delay after a spike\n",
    "\n",
    "# TODO: test this  ?????????????\n",
    "connections['XeAe'].connect(True) # all-to-all connection\n",
    "connections['XeAe'].delay = 'minDelay + rand() * deltaDelay'                       # rand() value between 0 and 1\n",
    "connections['XeAe'].w = weightMatrix[connections['XeAe'].i, connections['XeAe'].j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ae\n",
      "Ai\n",
      "Xe\n",
      "AeAi\n",
      "AiAe\n",
      "XeAe\n",
      "Ae\n",
      "Ai\n",
      "Xe\n",
      "Ae\n",
      "Ai\n",
      "Ae\n"
     ]
    }
   ],
   "source": [
    "net = Network()                                                                 # main simulation controller in brian2\n",
    "for obj_list in [neuron_groups, input_groups, connections, rate_monitors, spike_monitors, spike_counters]:\n",
    "    for key in obj_list:\n",
    "        print(key)\n",
    "        net.add(obj_list[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_spike_count = np.zeros(n_e)             # (225,)       number of spikes in Ae layer neurons\n",
    "assignments = np.zeros(n_e)                      # (225,)       ?????\n",
    "input_numbers = [0] * num_examples               # array 6000 long        label value (int 0 t0 9)    \n",
    "outputNumbers = np.zeros((num_examples, 10))     # (6000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_num = 1                                                               # first figure\n",
    "\n",
    "input_groups['Xe'].rates = 0 * Hz  \n",
    "\n",
    "net.run(0*second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "while j < (int(num_examples)):\n",
    "    spike_rates = testing['x'][j%dataset_size,:,:].reshape((n_input)) / 8. *  input_intensity  \n",
    "    # (28,28) => (784,)      divide by 4.  0>0Hz and 255>64.925Hz     # input_intensity is 2 ???\n",
    "    \n",
    "    input_groups['Xe'].rates = spike_rates * Hz                       # input_groups['Xe'] = possion group\n",
    "    print('run example number:', j+1, 'of', int(num_examples))\n",
    "    \n",
    "    net.run(single_example_time, report='text')   # 0.35 s\n",
    "\n",
    "    if j % update_interval == 0 and j > 0:                            # update_interval is 400\n",
    "        assignments = get_new_assignments(result_monitor[:], input_numbers[j-update_interval : j])\n",
    "        # return (225,) with each n_e is assigned with most spiking digit, according to the result monitor\n",
    "\n",
    "    current_spike_count = np.asarray(spike_counters['Ae'].count[:]) - previous_spike_count\n",
    "    previous_spike_count = np.copy(spike_counters['Ae'].count[:])\n",
    "    \n",
    "    if np.sum(current_spike_count) < 5:   # if not spiking for this image, increase intensity\n",
    "        print(\"only \", np.sum(current_spike_count), \" spikes for \", j+1, \" th example.\")\n",
    "        \n",
    "        input_intensity += 1              # why increase intensity ?\n",
    "        \n",
    "        input_groups['Xe'].rates = 0 * Hz\n",
    "        net.run(resting_time)                              # 150 ms\n",
    "    \n",
    "    else:                                                   \n",
    "        result_monitor[j%update_interval,:] = current_spike_count    # update_interval is 400\n",
    "        input_numbers[j] = testing['y'][j%dataset_size][0]\n",
    "        \n",
    "        outputNumbers[j,:] = get_recognized_number_ranking(assignments, result_monitor[j%update_interval,:])\n",
    "        # for each image, digits are ranked according to most number of spikes\n",
    "        \n",
    "        if j % 100 == 0 and j > 0:\n",
    "            print('runs done:', j, 'of', int(num_examples))\n",
    "            \n",
    "        if j % update_interval == 0 and j > 0:\n",
    "            unused, performance = update_performance_plot(performance_monitor, performance, j, fig_performance)\n",
    "            print('Classification performance', performance[:int(j/float(update_interval))+1])\n",
    "                \n",
    "        input_groups['Xe'].rates = 0 * Hz\n",
    "        net.run(resting_time)\n",
    "        input_intensity = start_input_intensity\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNN simulator",
   "language": "python",
   "name": "sim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
